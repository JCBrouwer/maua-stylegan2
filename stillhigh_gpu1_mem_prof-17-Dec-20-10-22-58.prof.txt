__main__ train:106                                :2087.1 Mb     if args.distributed:
+ __main__ train:106                                 (1, 3, 1, 1)         <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 512, 4, 4)       <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (128, 64, 1, 1)      <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 128, 256, 3, 3)  <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 3, 512, 1, 1)    <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 3, 256, 1, 1)    <class 'torch.Tensor'>
+ __main__ train:106                                 (64, 512)            <class 'torch.Tensor'>
+ __main__ train:106                                 (32,)                <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 128, 256, 3, 3)  <class 'torch.Tensor'>
+ __main__ train:106                                 (64, 64, 3, 3)       <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 3, 512, 1, 1)    <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 3, 256, 1, 1)    <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (256, 128, 3, 3)     <class 'torch.Tensor'>
+ __main__ train:106                                 (64, 512)            <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 1, 16, 16)       <class 'torch.Tensor'>
+ __main__ train:106                                 (32,)                <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (256, 128, 1, 1)     <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 32, 64, 3, 3)    <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 1, 8, 8)         <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 3, 32, 1, 1)     <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 32, 64, 3, 3)    <class 'torch.Tensor'>
+ __main__ train:106                                 (512, 513, 3, 3)     <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 3, 32, 1, 1)     <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (256, 128, 3, 3)     <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (64, 64, 3, 3)       <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 512)             <class 'torch.Tensor'>
+ __main__ train:106                                 (256, 128, 1, 1)     <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 32, 32, 3, 3)    <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 1, 4, 4)         <class 'torch.Tensor'>
+ __main__ train:106                                 (256, 512)           <class 'torch.Tensor'>
+ __main__ train:106                                 (512, 512)           <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 32, 32, 3, 3)    <class 'torch.Tensor'>
+ __main__ train:106                                 (32, 3, 1, 1)        <class 'torch.Tensor'>
+ __main__ train:106                                 (32, 32, 3, 3)       <class 'torch.Tensor'>
+ __main__ train:106                                 (512, 8192)          <class 'torch.Tensor'>
+ __main__ train:106                                 (256, 512)           <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 1, 64, 64)       <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 1, 128, 128)     <class 'torch.Tensor'>
+ __main__ train:106                                 (512, 512)           <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 1, 256, 256)     <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 1, 512, 512)     <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 1, 1024, 1024)   <class 'torch.Tensor'>
+ __main__ train:106                                 (512, 8192)          <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 512, 4, 4)       <class 'torch.Tensor'>
+ __main__ train:106                                 (128,)               <class 'torch.Tensor'>
+ __main__ train:106                                 (32, 512)            <class 'torch.Tensor'>
+ __main__ train:106                                 (64, 32, 3, 3)       <class 'torch.Tensor'>
+ __main__ train:106                                 (128,)               <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (32, 512)            <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (64, 32, 1, 1)       <class 'torch.Tensor'>
+ __main__ train:106                                 (32, 32, 3, 3)       <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 512, 512, 3, 3)  <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 256, 256, 3, 3)  <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 256, 512, 3, 3)  <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 128, 128, 3, 3)  <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 3, 128, 1, 1)    <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 64, 64, 3, 3)    <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 512)             <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (256,)               <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 512, 512, 3, 3)  <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 256, 256, 3, 3)  <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 256, 512, 3, 3)  <class 'torch.Tensor'>
+ __main__ train:106                                 (256, 256, 3, 3)     <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 3, 128, 1, 1)    <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (512, 512, 3, 3)     <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 128, 128, 3, 3)  <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 64, 64, 3, 3)    <class 'torch.Tensor'>
+ __main__ train:106                                 (128, 128, 3, 3)     <class 'torch.Tensor'>
+ __main__ train:106                                 (512, 256, 3, 3)     <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 1, 32, 32)       <class 'torch.Tensor'>
+ __main__ train:106                                 (512, 512, 1, 1)     <class 'torch.Tensor'>
+ __main__ train:106                                 (64, 32, 3, 3)       <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (512, 256, 1, 1)     <class 'torch.Tensor'>
+ __main__ train:106                                 (64, 32, 1, 1)       <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (64,)                <class 'torch.Tensor'>
+ __main__ train:106                                 (64,)                <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1,)                 <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (512, 256, 3, 3)     <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (128, 128, 3, 3)     <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (512, 512, 3, 3)     <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (256, 256, 3, 3)     <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (512, 512, 1, 1)     <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (512, 256, 1, 1)     <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 64, 128, 3, 3)   <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 3, 64, 1, 1)     <class 'torch.Tensor'>
+ __main__ train:106                                 (128, 512)           <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 64, 128, 3, 3)   <class 'torch.Tensor'>
+ __main__ train:106                                 (128, 64, 3, 3)      <class 'torch.Tensor'>
+ __main__ train:106                                 (1, 3, 64, 1, 1)     <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (128, 512)           <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (1, 3, 1, 1)         <class 'torch.Tensor'>
+ __main__ train:106                                 (128, 64, 1, 1)      <class 'torch.Tensor'>
+ __main__ train:106                                 (1,)                 <class 'torch.Tensor'>
+ __main__ train:106                                 (4, 4)               <class 'torch.Tensor'>
+ __main__ train:106                                 (512,)               <class 'torch.Tensor'>
+ __main__ train:106                                 (512, 513, 3, 3)     <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (256,)               <class 'torch.Tensor'>
+ __main__ train:106                                 (32, 3, 1, 1)        <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (512,)               <class 'torch.nn.parameter.Parameter'>
+ __main__ train:106                                 (128, 64, 3, 3)      <class 'torch.nn.parameter.Parameter'>
torch.tensor wrapped:24                           :4353.1 Mb         if not all(type(t) is Tensor for t in args) and has_torch_function(args):
+ torch.tensor wrapped:24                            (60, 512)            <class 'torch.Tensor'>
+ torch.tensor wrapped:24                            (1,)                 <class 'torch.Tensor'>
multiprocessing.queues _start_thread:175          :4353.1 Mb                 self._thread, Queue._finalize_join,
+ multiprocessing.queues _start_thread:175           (60, 512)            <class 'torch.Tensor'>
+ multiprocessing.queues _start_thread:175           (1,)                 <class 'torch.Tensor'>
enum __new__:535                                  :4353.1 Mb         if type(value) is cls:
+ enum __new__:535                                   (60, 512)            <class 'torch.Tensor'>
+ enum __new__:535                                   (1,)                 <class 'torch.Tensor'>
multiprocessing.connection _check_readable:139    :4353.1 Mb         if not self._readable:
+ multiprocessing.connection _check_readable:139     (60, 512)            <class 'torch.Tensor'>
+ multiprocessing.connection _check_readable:139     (1,)                 <class 'torch.Tensor'>
threading __init__:792                            :4353.1 Mb         self._kwargs = kwargs
+ threading __init__:792                             (60, 512)            <class 'torch.Tensor'>
+ threading __init__:792                             (1,)                 <class 'torch.Tensor'>
torchvision.transforms.functional normalize:278   :4353.1 Mb     if (std == 0).any():
+ torchvision.transforms.functional normalize:278    (1,)                 <class 'torch.Tensor'>
+ torchvision.transforms.functional normalize:278    (60, 512)            <class 'torch.Tensor'>
models.stylegan2 forward:250                      :4333.1 Mb             out = F.conv2d(inputs, weight, padding=self.padding, groups=batch)
+ models.stylegan2 forward:250                       (2, 512, 4, 4)       <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (60, 512)            <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1, 1024, 4, 4)      <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1,)                 <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 18, 512)         <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 512)             <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1024, 512, 3, 3)    <class 'torch.Tensor'>
torch.overrides has_torch_function:1087           :4353.1 Mb         for a in relevant_args
+ torch.overrides has_torch_function:1087            (1,)                 <class 'torch.Tensor'>
+ torch.overrides has_torch_function:1087            (60, 512)            <class 'torch.Tensor'>
torchvision.transforms.transforms forward:647     :4353.1 Mb         return img
+ torchvision.transforms.transforms forward:647      (60, 512)            <class 'torch.Tensor'>
+ torchvision.transforms.transforms forward:647      (1,)                 <class 'torch.Tensor'>
torch.nn.modules.module _call_impl:729            :2131.1 Mb                 _global_forward_hooks.values(),
models.stylegan2 forward:250                      :2129.1 Mb             out = F.conv2d(inputs, weight, padding=self.padding, groups=batch)
+ models.stylegan2 forward:250                       (6, 512, 1, 1)       <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1, 6, 4, 4)         <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1024, 512, 3, 3)    <class 'torch.Tensor'>
torch.overrides <genexpr>:1084                    :2131.1 Mb         type(a) is not torch.Tensor and
threading daemon:1128                             :2131.1 Mb         assert self._initialized, "Thread.__init__() not called"
selectors __init__:211                            :2131.1 Mb         self._fd_to_key = {}
enum __new__:535                                  :2131.1 Mb         if type(value) is cls:
torch.overrides <genexpr>:1087                    :2131.1 Mb         for a in relevant_args
torch.tensor <genexpr>:24                         :2131.1 Mb         if not all(type(t) is Tensor for t in args) and has_torch_function(args):
multiprocessing.util __init__:199                 :2131.1 Mb         self._kwargs = kwargs or {}
torch.nn.modules.module _call_impl:730            :4353.1 Mb                 self._forward_hooks.values()):
torch.overrides <genexpr>:1084                    :4353.1 Mb         type(a) is not torch.Tensor and
multiprocessing.util __init__:192                 :4353.1 Mb         if obj is not None:
torch.nn.modules.module _call_impl:729            :4353.1 Mb                 _global_forward_hooks.values(),
models.stylegan2 forward:235                      :4333.1 Mb             out = F.conv_transpose2d(inputs, weight, padding=0, stride=2, groups=batch)
+ models.stylegan2 forward:235                       (2, 3, 4, 4)         <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (1024, 512, 3, 3)    <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (2, 512)             <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (1, 1024, 4, 4)      <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (1, 1024, 9, 9)      <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (6, 512, 1, 1)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1, 6, 4, 4)         <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1, 1024, 4, 4)      <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
torchvision.transforms.transforms forward:615     :4353.1 Mb         if torch.rand(1) < self.p:
importlib._bootstrap <genexpr>:321                :4353.1 Mb             if any(arg is not None for arg in args):
selectors _fileobj_lookup:224                     :4353.1 Mb         try:
threading __init__:226                            :4353.1 Mb         try:
torch.nn.modules.module _call_impl:749            :6521.1 Mb         return result
importlib._bootstrap _handle_fromlist:1044        :6521.1 Mb     return module
selectors _fileobj_to_fd:36                       :6521.1 Mb         try:
torchvision.transforms.functional normalize:281   :6521.1 Mb         mean = mean.view(-1, 1, 1)
multiprocessing.util __init__:198                 :6521.1 Mb         self._args = args
torchvision.transforms.transforms __call__:66     :6521.1 Mb         for t in self.transforms:
threading __init__:229                            :6521.1 Mb             pass
importlib._bootstrap __exit__:327                 :6521.1 Mb                 _verbose_message('import {!r} # {!r}', spec.name, spec.loader)
models.stylegan2 forward:250                      :6501.1 Mb             out = F.conv2d(inputs, weight, padding=self.padding, groups=batch)
+ models.stylegan2 forward:250                       (2, 512, 8, 8)       <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1024, 512, 3, 3)    <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1, 1024, 8, 8)      <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (1024, 512, 3, 3)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (2, 512)             <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (1, 1024, 4, 4)      <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (1, 1024, 9, 9)      <class 'torch.Tensor'>
torch.utils.data._utils.collate default_collate:47:2131.1 Mb     if isinstance(elem, torch.Tensor):
models.stylegan2 forward:250                      :2129.1 Mb             out = F.conv2d(inputs, weight, padding=self.padding, groups=batch)
+ models.stylegan2 forward:250                       (6, 512, 1, 1)       <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1, 6, 8, 8)         <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1024, 512, 3, 3)    <class 'torch.Tensor'>
torch.overrides <genexpr>:1087                    :2131.1 Mb         for a in relevant_args
threading __init__:233                            :2131.1 Mb             pass
selectors _fileobj_to_fd:43                       :2131.1 Mb     return fd
importlib._bootstrap _find_and_load_unlocked:968  :2131.1 Mb     if parent:
multiprocessing.util __init__:203                 :2131.1 Mb         _finalizer_registry[self._key] = self
torch.nn.modules.module _call_impl:729            :2131.1 Mb                 _global_forward_hooks.values(),
torch.nn.modules.module _call_impl:718            :2131.1 Mb                 self._forward_pre_hooks.values()):
torch.overrides <genexpr>:1084                    :4353.1 Mb         type(a) is not torch.Tensor and
multiprocessing.queues put:89                     :4353.1 Mb             self._notempty.notify()
torch.nn.modules.module _call_impl:734            :4353.1 Mb         if (len(self._backward_hooks) > 0) or (len(_global_backward_hooks) > 0):
threading __init__:235                            :4353.1 Mb             self._is_owned = lock._is_owned
importlib._bootstrap _find_and_load_unlocked:971  :4353.1 Mb         setattr(parent_module, name.rpartition('.')[2], module)
torch.utils.data._utils.worker get_worker_info:109:4353.1 Mb     return _worker_info
torch.nn.modules.module _call_impl:727            :4353.1 Mb             result = self.forward(*input, **kwargs)
selectors register:244                            :4353.1 Mb         self._fd_to_key[key.fd] = key
models.stylegan2 forward:235                      :4333.1 Mb             out = F.conv_transpose2d(inputs, weight, padding=0, stride=2, groups=batch)
+ models.stylegan2 forward:235                       (1024, 512, 3, 3)    <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (1, 1024, 17, 17)    <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (1, 1024, 8, 8)      <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (2, 512)             <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (2, 3, 8, 8)         <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (2, 3, 4, 4)         <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (6, 512, 1, 1)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1, 6, 8, 8)         <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1, 1024, 8, 8)      <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
torch.utils.data._utils.collate default_collate:53:8715.1 Mb             storage = elem.storage()._new_shared(numel)
threading __init__:800                            :8715.1 Mb         self._is_stopped = False
torch.nn.modules.module _call_impl:749            :8715.1 Mb         return result
importlib._bootstrap release:106                  :8715.1 Mb             if self.owner != tid:
selectors register:356                            :8715.1 Mb         if events & EVENT_WRITE:
torch.utils.data._utils.fetch <listcomp>:44       :8715.1 Mb             data = [self.dataset[idx] for idx in possibly_batched_index]
torch.tensor wrapped:24                           :8715.1 Mb         if not all(type(t) is Tensor for t in args) and has_torch_function(args):
threading notify:348                              :8715.1 Mb         waiters_to_notify = _deque(_islice(all_waiters, n))
models.stylegan2 forward:250                      :8697.1 Mb             out = F.conv2d(inputs, weight, padding=self.padding, groups=batch)
+ models.stylegan2 forward:250                       (2, 512, 16, 16)     <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1024, 512, 3, 3)    <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1, 1024, 16, 16)    <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (1024, 512, 3, 3)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (1, 1024, 17, 17)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (1, 1024, 8, 8)      <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (2, 512)             <class 'torch.Tensor'>
importlib._bootstrap _handle_fromlist:1020        :6513.1 Mb         for x in fromlist:
models.stylegan2 forward:250                      :2133.1 Mb             out = F.conv2d(inputs, weight, padding=self.padding, groups=batch)
+ models.stylegan2 forward:250                       (6, 512, 1, 1)       <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1, 6, 16, 16)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1024, 512, 3, 3)    <class 'torch.Tensor'>
multiprocessing.queues _start_thread:167          :6513.1 Mb         self._thread.daemon = True
torch.utils.data._utils.worker _worker_loop:169   :6513.1 Mb         while watchdog.is_alive():
importlib._bootstrap cb:177                       :6513.1 Mb                 _imp.acquire_lock()
multiprocessing.connection wait:918               :6513.1 Mb                 deadline = time.monotonic() + timeout
dataset __getitem__:31                            :6513.1 Mb                     img_bytes = txn.get(key)
torchvision.transforms.functional_pil _is_pil_image:19:6513.1 Mb         return isinstance(img, Image.Image)
torch.overrides has_torch_function:1087           :6513.1 Mb         for a in relevant_args
importlib._bootstrap _handle_fromlist:1044        :2133.1 Mb     return module
dataset __getitem__:33                            :2133.1 Mb                 buffer = BytesIO(img_bytes)
torch.overrides <genexpr>:1084                    :4363.1 Mb         type(a) is not torch.Tensor and
models.stylegan2 forward:235                      :4345.1 Mb             out = F.conv_transpose2d(inputs, weight, padding=0, stride=2, groups=batch)
+ models.stylegan2 forward:235                       (1024, 512, 3, 3)    <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (1, 1024, 33, 33)    <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (1, 1024, 16, 16)    <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (2, 512)             <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (2, 3, 16, 16)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (6, 512, 1, 1)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1, 6, 16, 16)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1, 1024, 16, 16)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (2, 3, 8, 8)         <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
multiprocessing.connection wait:920               :2133.1 Mb             while True:
importlib._bootstrap cb:178                       :2133.1 Mb                 try:
threading daemon:1133                             :2133.1 Mb         if not self._initialized:
torch.utils.data._utils.worker is_alive:55        :2133.1 Mb             if not self.manager_dead:
torchvision.transforms.functional to_tensor:66    :2133.1 Mb     if _is_numpy(pic) and not _is_numpy_image(pic):
dataset __getitem__:34                            :4345.1 Mb                 img = Image.open(buffer)
torch.storage _new_shared:132                     :4345.1 Mb         if cls.is_cuda:
multiprocessing.connection wait:921               :4345.1 Mb                 ready = selector.select(timeout)
importlib._bootstrap cb:182                       :6539.1 Mb                     if _module_locks.get(name) is ref:
models.stylegan2 forward:250                      :6521.1 Mb             out = F.conv2d(inputs, weight, padding=self.padding, groups=batch)
+ models.stylegan2 forward:250                       (2, 512, 32, 32)     <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1024, 512, 3, 3)    <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1, 1024, 32, 32)    <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (1024, 512, 3, 3)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (1, 1024, 33, 33)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (1, 1024, 16, 16)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (2, 512)             <class 'torch.Tensor'>
torchvision.transforms.functional _is_numpy:44    :6521.1 Mb     return isinstance(img, np.ndarray)
torch.utils.data._utils.worker is_alive:56        :6521.1 Mb                 self.manager_dead = os.getppid() != self.manager_pid
threading daemon:1135                             :6539.1 Mb         if self._started.is_set():
torch.tensor wrapped:27                           :6539.1 Mb             return f(*args, **kwargs)
torch.storage _new_shared:134                     :6521.1 Mb         elif get_sharing_strategy() == 'file_system':
selectors select:405                              :6521.1 Mb         if timeout is None:
dataset __getitem__:35                            :6521.1 Mb                 break
models.stylegan2 forward:250                      :4327.1 Mb             out = F.conv2d(inputs, weight, padding=self.padding, groups=batch)
+ models.stylegan2 forward:250                       (6, 512, 1, 1)       <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1, 6, 32, 32)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1024, 512, 3, 3)    <class 'torch.Tensor'>
importlib._bootstrap _handle_fromlist:1020        :4327.1 Mb         for x in fromlist:
torchvision.transforms.functional to_tensor:69    :4327.1 Mb     if isinstance(pic, np.ndarray):
torch.utils.data._utils.worker is_alive:57        :4327.1 Mb             return not self.manager_dead
threading is_set:509                              :4327.1 Mb         return self._flag
torchvision.transforms.transforms forward:616     :4327.1 Mb             return F.hflip(img)
torch.multiprocessing get_sharing_strategy:70     :4327.1 Mb     return _sharing_strategy
selectors select:407                              :4327.1 Mb         elif timeout <= 0:
dataset __getitem__:40                            :4327.1 Mb         img = self.transform(img)
importlib._bootstrap _handle_fromlist:1044        :6521.1 Mb     return module
torchvision.transforms.functional to_tensor:81    :6521.1 Mb     if accimage is not None and isinstance(pic, accimage.Image):
threading daemon:1137                             :6521.1 Mb         self._daemonic = daemonic
torch.utils.data._utils.worker _worker_loop:170   :6521.1 Mb             try:
torchvision.transforms.functional hflip:457       :6521.1 Mb     if not isinstance(img, torch.Tensor):
models.stylegan2 forward:235                      :6521.1 Mb             out = F.conv_transpose2d(inputs, weight, padding=0, stride=2, groups=batch)
+ models.stylegan2 forward:235                       (1024, 512, 3, 3)    <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (1, 1024, 65, 65)    <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (1, 1024, 32, 32)    <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (2, 512)             <class 'torch.Tensor'>
+ models.stylegan2 forward:235                       (2, 3, 32, 32)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (6, 512, 1, 1)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1, 6, 32, 32)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1, 1024, 32, 32)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (2, 3, 16, 16)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
torch.storage _new_shared:137                     :6521.1 Mb             return cls._new_using_fd(size)
selectors select:412                              :6521.1 Mb             timeout = math.ceil(timeout * 1e3)
torchvision.transforms.transforms __call__:66     :6521.1 Mb         for t in self.transforms:
models.stylegan2 forward:250                      :4327.1 Mb             out = F.conv2d(inputs, weight, padding=self.padding, groups=batch)
+ models.stylegan2 forward:250                       (6, 512, 1, 1)       <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1, 6, 64, 64)       <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1, 1024, 64, 64)    <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 512, 64, 64)     <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (1024, 512, 3, 3)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (1, 1024, 65, 65)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (1, 1024, 32, 32)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (2, 512)             <class 'torch.Tensor'>
multiprocessing.util debug:49                     :4327.1 Mb     if _logger:
torchvision.transforms.transforms forward:647     :4327.1 Mb         return img
torchvision.transforms.functional to_tensor:63    :4327.1 Mb     if not(F_pil._is_pil_image(pic) or _is_numpy(pic)):
selectors __init__:213                            :4327.1 Mb         self._map = _SelectorMapping(self)
torchvision.transforms.transforms __call__:66     :4327.1 Mb         for t in self.transforms:
torch.utils.data._utils.collate default_collate:54:4327.1 Mb             out = elem.new(storage)
selectors select:413                              :4327.1 Mb         ready = []
torchvision.transforms.transforms __call__:67     :4327.1 Mb             img = t(img)
torchvision.transforms.functional to_tensor:101   :6649.1 Mb     if isinstance(img, torch.ByteTensor):
torchvision.transforms.functional normalize:275   :6649.1 Mb     dtype = tensor.dtype
multiprocessing.connection fileno:170             :6649.1 Mb         self._check_closed()
multiprocessing.queues _start_thread:182          :6649.1 Mb             self, Queue._finalize_close,
importlib._bootstrap _handle_fromlist:1019        :6649.1 Mb     if hasattr(module, '__path__'):
torch.utils.data._utils.collate default_collate:55:6649.1 Mb         return torch.stack(batch, 0, out=out)
selectors select:414                              :6649.1 Mb         try:
torch.nn.modules.module _call_impl:716            :6649.1 Mb         for hook in itertools.chain(
models.stylegan2 forward:250                      :6649.1 Mb             out = F.conv2d(inputs, weight, padding=self.padding, groups=batch)
+ models.stylegan2 forward:250                       (2, 3, 64, 64)       <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 256)             <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1, 512, 128, 128)   <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 256, 128, 128)   <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 1, 256, 1, 1)    <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (512, 256, 3, 3)     <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (6, 512, 1, 1)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1, 6, 64, 64)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1, 1024, 64, 64)    <class 'torch.Tensor'>
- models.stylegan2 forward:235                       (2, 3, 32, 32)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 1, 512, 1, 1)    <class 'torch.Tensor'>
models.stylegan2 forward:250                      :4327.1 Mb             out = F.conv2d(inputs, weight, padding=self.padding, groups=batch)
+ models.stylegan2 forward:250                       (6, 256, 1, 1)       <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1, 6, 128, 128)     <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (512, 256, 3, 3)     <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 256)             <class 'torch.Tensor'>
multiprocessing.connection wait:918               :4327.1 Mb                 deadline = time.monotonic() + timeout
multiprocessing.queues _start_thread:159          :4327.1 Mb         self._buffer.clear()
torch.tensor wrapped:27                           :4327.1 Mb             return f(*args, **kwargs)
threading notify:347                              :4327.1 Mb         all_waiters = self._waiters
selectors select:426                              :4327.1 Mb             if key:
importlib._bootstrap _handle_fromlist:1044        :4327.1 Mb     return module
torch.tensor wrapped:23                           :4327.1 Mb         from torch.overrides import has_torch_function, handle_torch_function
torch.nn.modules.module _call_impl:728            :4327.1 Mb         for hook in itertools.chain(
torch.utils.data._utils.worker _worker_loop:171   :8941.1 Mb                 r = index_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
selectors close:270                               :8941.1 Mb         self._map = None
torch.overrides <genexpr>:1087                    :8941.1 Mb         for a in relevant_args
selectors select:423                              :8941.1 Mb                 events |= EVENT_READ
torchvision.transforms.functional to_tensor:63    :8941.1 Mb     if not(F_pil._is_pil_image(pic) or _is_numpy(pic)):
threading __init__:792                            :8941.1 Mb         self._kwargs = kwargs
torch.nn.modules.module _call_impl:749            :8941.1 Mb         return result
torch.overrides has_torch_function:1087           :8941.1 Mb         for a in relevant_args
models.stylegan2 forward:250                      :8941.1 Mb             out = F.conv2d(inputs, weight, padding=self.padding, groups=batch)
+ models.stylegan2 forward:250                       (2, 3, 128, 128)     <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 128)             <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1, 256, 256, 256)   <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 128, 256, 256)   <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 1, 128, 1, 1)    <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (256, 128, 3, 3)     <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (6, 256, 1, 1)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 3, 64, 64)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1, 6, 128, 128)     <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1, 512, 128, 128)   <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 1, 256, 1, 1)    <class 'torch.Tensor'>
models.stylegan2 forward:250                      :4327.1 Mb             out = F.conv2d(inputs, weight, padding=self.padding, groups=batch)
+ models.stylegan2 forward:250                       (6, 128, 1, 1)       <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1, 6, 256, 256)     <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (256, 128, 3, 3)     <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 128)             <class 'torch.Tensor'>
torchvision.transforms.functional normalize:283   :4327.1 Mb         std = std.view(-1, 1, 1)
threading __init__:221                            :4327.1 Mb         self.acquire = lock.acquire
selectors __init__:213                            :4327.1 Mb         self._map = _SelectorMapping(self)
multiprocessing.connection _recv:380              :4327.1 Mb             n = len(chunk)
torchvision.transforms.functional to_tensor:98    :4327.1 Mb     img = img.view(pic.size[1], pic.size[0], len(pic.getbands()))
multiprocessing.connection _poll:415              :4327.1 Mb         return bool(r)
dataset __getitem__:40                            :4327.1 Mb         img = self.transform(img)
torch.nn.modules.module _call_impl:724            :4327.1 Mb         if torch._C._get_tracing_state():
threading __init__:789                            :6631.1 Mb         self._target = target
dataset __getitem__:27                            :6631.1 Mb         while True:
selectors close:269                               :6631.1 Mb         self._fd_to_key.clear()
torchvision.transforms.transforms __call__:68     :6631.1 Mb         return img
torch.nn.modules.module _call_impl:727            :6631.1 Mb             result = self.forward(*input, **kwargs)
multiprocessing.util __init__:201                 :6631.1 Mb         self._pid = os.getpid()
torch.nn.modules.module _call_impl:729            :6631.1 Mb                 _global_forward_hooks.values(),
torch.nn.modules.module _call_impl:727            :6631.1 Mb             result = self.forward(*input, **kwargs)
models.stylegan2 forward:250                      :6631.1 Mb             out = F.conv2d(inputs, weight, padding=self.padding, groups=batch)
+ models.stylegan2 forward:250                       (2, 3, 512, 512)     <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 32)              <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1, 64, 1024, 1024)  <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 32, 1024, 1024)  <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 64, 512, 512)    <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (2, 1, 32, 1, 1)     <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (64, 32, 3, 3)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (6, 128, 1, 1)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 3, 128, 128)     <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1, 6, 256, 256)     <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1, 256, 256, 256)   <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 1, 128, 1, 1)    <class 'torch.Tensor'>
models.stylegan2 forward:250                      :4327.1 Mb             out = F.conv2d(inputs, weight, padding=self.padding, groups=batch)
+ models.stylegan2 forward:250                       (6, 32, 1, 1)        <class 'torch.Tensor'>
+ models.stylegan2 forward:250                       (1, 6, 1024, 1024)   <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (64, 32, 3, 3)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 32)              <class 'torch.Tensor'>
torch.nn.modules.module _call_impl:727            :4327.1 Mb             result = self.forward(*input, **kwargs)
torchvision.transforms.functional to_tensor:91    :4327.1 Mb     elif pic.mode == 'F':
threading __init__:226                            :4327.1 Mb         try:
torch.overrides has_torch_function:1084           :4327.1 Mb         type(a) is not torch.Tensor and
multiprocessing.connection _recv:386              :4327.1 Mb             buf.write(chunk)
torch.overrides <genexpr>:1084                    :4327.1 Mb         type(a) is not torch.Tensor and
importlib._bootstrap _handle_fromlist:1019        :4327.1 Mb     if hasattr(module, '__path__'):
threading notify:345                              :4327.1 Mb         if not self._is_owned():
torchvision.transforms.transforms forward:615     :6627.1 Mb         if torch.rand(1) < self.p:
multiprocessing.synchronize __enter__:95          :6627.1 Mb         return self._semlock.__enter__()
threading is_set:509                              :6627.1 Mb         return self._flag
torch.utils.data._utils.collate default_collate:48:6627.1 Mb         out = None
multiprocessing.connection poll:256               :6627.1 Mb         self._check_readable()
torch.nn.modules.module _call_impl:727            :6627.1 Mb             result = self.forward(*input, **kwargs)
torch.tensor wrapped:24                           :6627.1 Mb         if not all(type(t) is Tensor for t in args) and has_torch_function(args):
torch.nn.modules.module __getattr__:769           :6627.1 Mb                 return _parameters[name]
+ torch.nn.modules.module __getattr__:769            (2, 32, 1024, 1024)  <class 'torch.Tensor'>
+ torch.nn.modules.module __getattr__:769            (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 512, 4, 4)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 512, 8, 8)       <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (6, 32, 1, 1)        <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1, 6, 1024, 1024)   <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 3, 512, 512)     <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (1, 64, 1024, 1024)  <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 512, 64, 64)     <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 128, 256, 256)   <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 256, 128, 128)   <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 32, 1024, 1024)  <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 64, 512, 512)    <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 512, 32, 32)     <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 512, 16, 16)     <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 1, 32, 1, 1)     <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 18, 512)         <class 'torch.Tensor'>
torch.overrides <genexpr>:1084                    :6627.1 Mb         type(a) is not torch.Tensor and
multiprocessing.util __init__:187                 :4323.1 Mb         if (exitpriority is not None) and not isinstance(exitpriority,int):
dataset __getitem__:35                            :4323.1 Mb                 break
torch.nn.modules.module __getattr__:769           :4323.1 Mb                 return _parameters[name]
+ torch.nn.modules.module __getattr__:769            (2, 32, 1025, 1025)  <class 'torch.Tensor'>
+ torch.nn.modules.module __getattr__:769            (2, 64, 512, 512)    <class 'torch.Tensor'>
torch.multiprocessing get_sharing_strategy:70     :4323.1 Mb     return _sharing_strategy
torchvision.transforms.functional_pil _is_pil_image:16:4323.1 Mb     if accimage is not None:
torch.overrides <genexpr>:1084                    :4323.1 Mb         type(a) is not torch.Tensor and
selectors register:235                            :4323.1 Mb         if (not events) or (events & ~(EVENT_READ | EVENT_WRITE)):
torch.tensor wrapped:26                           :4323.1 Mb         try:
torchvision.transforms.functional_pil hflip:51    :4323.1 Mb     if not _is_pil_image(img):
torch.nn.modules.module _call_impl:716            :8933.1 Mb         for hook in itertools.chain(
torchvision.transforms.functional_pil _is_pil_image:16:8933.1 Mb     if accimage is not None:
multiprocessing.util __init__:198                 :8933.1 Mb         self._args = args
torch.nn.modules.module _call_impl:728            :8933.1 Mb         for hook in itertools.chain(
torchvision.transforms.functional normalize:280   :8933.1 Mb     if mean.ndim == 1:
selectors _fileobj_lookup:225                     :8933.1 Mb             return _fileobj_to_fd(fileobj)
torch.overrides <genexpr>:1087                    :8933.1 Mb         for a in relevant_args
torch.utils.data._utils.collate default_collate:55:8933.1 Mb         return torch.stack(batch, 0, out=out)
torch.nn.modules.module __getattr__:769           :8933.1 Mb                 return _parameters[name]
- torch.nn.modules.module __getattr__:769            (2, 32, 1025, 1025)  <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 32, 1024, 1024)  <class 'torch.Tensor'>
torchvision.transforms.functional to_tensor:81    :4323.1 Mb     if accimage is not None and isinstance(pic, accimage.Image):
multiprocessing.queues _start_thread:161          :4323.1 Mb             target=Queue._feed,
torch.nn.modules.module __getattr__:769           :4323.1 Mb                 return _parameters[name]
+ torch.nn.modules.module __getattr__:769            (2, 64, 513, 513)    <class 'torch.Tensor'>
+ torch.nn.modules.module __getattr__:769            (2, 128, 256, 256)   <class 'torch.Tensor'>
torch.overrides <genexpr>:1087                    :4323.1 Mb         for a in relevant_args
torchvision.transforms.functional_pil _is_pil_image:19:4323.1 Mb         return isinstance(img, Image.Image)
torchvision.transforms.transforms __call__:104    :4323.1 Mb         return F.to_tensor(pic)
selectors register:355                            :4323.1 Mb             poller_events |= self._EVENT_READ
multiprocessing.util __init__:200                 :4323.1 Mb         self._key = (exitpriority, next(_finalizer_counter))
dataset __getitem__:42                            :4323.1 Mb         return img
multiprocessing.queues _start_thread:165          :6631.1 Mb             name='QueueFeederThread'
torch.utils.data._utils.fetch fetch:47            :6631.1 Mb         return self.collate_fn(data)
torchvision.transforms.functional to_tensor:66    :6631.1 Mb     if _is_numpy(pic) and not _is_numpy_image(pic):
torchvision.transforms.functional to_tensor:93    :6631.1 Mb     elif pic.mode == '1':
torchvision.transforms.functional to_tensor:81    :6631.1 Mb     if accimage is not None and isinstance(pic, accimage.Image):
torch.tensor wrapped:26                           :6631.1 Mb         try:
selectors register:363                            :6631.1 Mb         return key
multiprocessing.util __init__:203                 :6631.1 Mb         _finalizer_registry[self._key] = self
torch.nn.modules.module __getattr__:769           :6631.1 Mb                 return _parameters[name]
- torch.nn.modules.module __getattr__:769            (2, 64, 513, 513)    <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 64, 512, 512)    <class 'torch.Tensor'>
torch.utils.data._utils.worker _worker_loop:213   :4325.1 Mb             del data, idx, index, r  # save memory
torch.nn.modules.module __getattr__:769           :4325.1 Mb                 return _parameters[name]
+ torch.nn.modules.module __getattr__:769            (2, 128, 255, 255)   <class 'torch.Tensor'>
+ torch.nn.modules.module __getattr__:769            (2, 256, 128, 128)   <class 'torch.Tensor'>
selectors select:420                              :4325.1 Mb             if event & ~self._EVENT_READ:
torch.nn.modules.module _call_impl:727            :4325.1 Mb             result = self.forward(*input, **kwargs)
threading __init__:798                            :4325.1 Mb         self._tstate_lock = None
importlib._bootstrap _handle_fromlist:1021        :4325.1 Mb             if not isinstance(x, str):
torch.nn.modules.module _call_impl:717            :4325.1 Mb                 _global_forward_pre_hooks.values(),
torchvision.transforms.functional normalize:265   :4325.1 Mb     if not isinstance(tensor, torch.Tensor):
torchvision.transforms.functional to_tensor:101   :4325.1 Mb     if isinstance(img, torch.ByteTensor):
torch.utils.data._utils.worker is_alive:55        :5487.1 Mb             if not self.manager_dead:
selectors select:422                              :5487.1 Mb             if event & ~self._EVENT_WRITE:
torchvision.transforms.transforms forward:615     :5487.1 Mb         if torch.rand(1) < self.p:
torch.nn.modules.module __getattr__:769           :5487.1 Mb                 return _parameters[name]
- torch.nn.modules.module __getattr__:769            (2, 128, 255, 255)   <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 128, 256, 256)   <class 'torch.Tensor'>
threading __init__:799                            :5487.1 Mb         self._started = Event()
importlib._bootstrap _handle_fromlist:1028        :5487.1 Mb             elif x == '*':
torch.nn.modules.module _call_impl:718            :5487.1 Mb                 self._forward_pre_hooks.values()):
torchvision.transforms.functional normalize:268   :5487.1 Mb     if tensor.ndim < 3:
torchvision.transforms.functional to_tensor:102   :5487.1 Mb         return img.float().div(255)
torchvision.transforms.functional normalize:278   :6593.1 Mb     if (std == 0).any():
selectors __init__:348                            :6593.1 Mb         super().__init__()
torch.overrides <genexpr>:1087                    :6593.1 Mb         for a in relevant_args
torchvision.transforms.transforms forward:226     :6593.1 Mb         return F.normalize(tensor, self.mean, self.std, self.inplace)
multiprocessing.connection _check_closed:135      :6593.1 Mb         if self._handle is None:
importlib._bootstrap _handle_fromlist:1044        :6593.1 Mb     return module
torch.utils.data._utils.worker _worker_loop:212   :6593.1 Mb             data_queue.put((idx, data))
threading __init__:231                            :6593.1 Mb             self._acquire_restore = lock._acquire_restore
torch.nn.modules.module __getattr__:769           :6593.1 Mb                 return _parameters[name]
+ torch.nn.modules.module __getattr__:769            (2, 512, 64, 64)     <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 256, 128, 128)   <class 'torch.Tensor'>
multiprocessing.connection _recv:381              :5487.1 Mb             if n == 0:
torch.nn.modules.module __getattr__:769           :5487.1 Mb                 return _parameters[name]
+ torch.nn.modules.module __getattr__:769            (2, 512, 63, 63)     <class 'torch.Tensor'>
+ torch.nn.modules.module __getattr__:769            (2, 512, 32, 32)     <class 'torch.Tensor'>
torch.tensor wrapped:24                           :5487.1 Mb         if not all(type(t) is Tensor for t in args) and has_torch_function(args):
selectors _fileobj_to_fd:33                       :5487.1 Mb     if isinstance(fileobj, int):
torch.overrides <genexpr>:1087                    :5487.1 Mb         for a in relevant_args
threading __init__:804                            :5487.1 Mb         self._stderr = _sys.stderr
multiprocessing.queues _start_thread:162          :5487.1 Mb             args=(self._buffer, self._notempty, self._send_bytes,
torch.tensor wrapped:26                           :5487.1 Mb         try:
torch.nn.modules.module _call_impl:728            :5487.1 Mb         for hook in itertools.chain(
torch.tensor wrapped:27                           :6593.1 Mb             return f(*args, **kwargs)
multiprocessing.queues _start_thread:163          :6593.1 Mb                   self._wlock, self._writer.close, self._ignore_epipe,
torch.nn.modules.module _call_impl:729            :6593.1 Mb                 _global_forward_hooks.values(),
torch.tensor <genexpr>:24                         :6593.1 Mb         if not all(type(t) is Tensor for t in args) and has_torch_function(args):
threading __init__:806                            :6593.1 Mb         _dangling.add(self)
selectors _fileobj_to_fd:36                       :6593.1 Mb         try:
torch.overrides <genexpr>:1084                    :6593.1 Mb         type(a) is not torch.Tensor and
torch.nn.modules.module __getattr__:769           :6593.1 Mb                 return _parameters[name]
- torch.nn.modules.module __getattr__:769            (2, 512, 63, 63)     <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 512, 64, 64)     <class 'torch.Tensor'>
multiprocessing.connection _recv:387              :6593.1 Mb             remaining -= n
torchvision.transforms.transforms __call__:104    :5491.1 Mb         return F.to_tensor(pic)
multiprocessing.connection _recv_bytes:411        :5491.1 Mb         return self._recv(size)
torchvision.transforms.functional normalize:283   :5491.1 Mb         std = std.view(-1, 1, 1)
selectors _fileobj_to_fd:43                       :5491.1 Mb     return fd
threading __init__:790                            :5491.1 Mb         self._name = str(name or _newname())
threading daemon:1137                             :5491.1 Mb         self._daemonic = daemonic
torch.nn.modules.module __getattr__:769           :5491.1 Mb                 return _parameters[name]
+ torch.nn.modules.module __getattr__:769            (2, 512, 31, 31)     <class 'torch.Tensor'>
+ torch.nn.modules.module __getattr__:769            (2, 512, 16, 16)     <class 'torch.Tensor'>
torch.nn.modules.module _call_impl:728            :5491.1 Mb         for hook in itertools.chain(
torch.overrides <genexpr>:1084                    :5491.1 Mb         type(a) is not torch.Tensor and
multiprocessing.connection _recv:377              :7687.1 Mb         remaining = size
torchvision.transforms.functional normalize:285   :7687.1 Mb     return tensor
torch.overrides <genexpr>:1087                    :7687.1 Mb         for a in relevant_args
selectors register:244                            :7687.1 Mb         self._fd_to_key[key.fd] = key
torch.nn.modules.module _call_impl:729            :7687.1 Mb                 _global_forward_hooks.values(),
multiprocessing.util debug:49                     :7687.1 Mb     if _logger:
threading __init__:792                            :7687.1 Mb         self._kwargs = kwargs
torchvision.transforms.functional_pil _is_pil_image:19:7687.1 Mb         return isinstance(img, Image.Image)
torch.nn.modules.module __getattr__:769           :7685.1 Mb                 return _parameters[name]
- torch.nn.modules.module __getattr__:769            (2, 512, 31, 31)     <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 512, 32, 32)     <class 'torch.Tensor'>
threading __init__:797                            :5495.1 Mb         self._ident = None
torchvision.transforms.functional normalize:284   :5495.1 Mb     tensor.sub_(mean).div_(std)
torch.utils.data._utils.fetch <listcomp>:44       :5495.1 Mb             data = [self.dataset[idx] for idx in possibly_batched_index]
torchvision.transforms.functional to_tensor:91    :5495.1 Mb     elif pic.mode == 'F':
dataset __getitem__:27                            :5495.1 Mb         while True:
torch.nn.modules.module __getattr__:769           :5495.1 Mb                 return _parameters[name]
+ torch.nn.modules.module __getattr__:769            (2, 512, 15, 15)     <class 'torch.Tensor'>
+ torch.nn.modules.module __getattr__:769            (2, 512, 8, 8)       <class 'torch.Tensor'>
multiprocessing.connection wait:914               :5495.1 Mb             for obj in object_list:
multiprocessing.connection recv_bytes:217         :5495.1 Mb         if buf is None:
threading start:852                               :5495.1 Mb             _start_new_thread(self._bootstrap, ())
threading __init__:219                            :6593.1 Mb         self._lock = lock
multiprocessing.connection wait:917               :6593.1 Mb             if timeout is not None:
dataset __getitem__:28                            :6593.1 Mb             try:
torchvision.transforms.functional to_tensor:96    :6593.1 Mb         img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))
multiprocessing.connection recv_bytes:219         :6593.1 Mb         return buf.getvalue()
dataset __getitem__:30                            :6593.1 Mb                     key = f"{self.resolution}-{str(index).zfill(5)}".encode("utf-8")
torch.nn.modules.module _call_impl:734            :6593.1 Mb         if (len(self._backward_hooks) > 0) or (len(_global_backward_hooks) > 0):
torch.nn.modules.module __getattr__:769           :6593.1 Mb                 return _parameters[name]
- torch.nn.modules.module __getattr__:769            (2, 512, 15, 15)     <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 512, 16, 16)     <class 'torch.Tensor'>
threading start:857                               :6593.1 Mb         self._started.wait()
torch.nn.modules.module __getattr__:769           :5495.1 Mb                 return _parameters[name]
+ torch.nn.modules.module __getattr__:769            (2, 512, 7, 7)       <class 'torch.Tensor'>
+ torch.nn.modules.module __getattr__:769            (2, 512, 4, 4)       <class 'torch.Tensor'>
torch.nn.modules.module _call_impl:718            :5495.1 Mb                 self._forward_pre_hooks.values()):
torch.utils.data._utils.collate default_collate:48:5495.1 Mb         out = None
multiprocessing.queues _start_thread:171          :5495.1 Mb         debug('... done self._thread.start()')
torch.nn.modules.module _call_impl:718            :5495.1 Mb                 self._forward_pre_hooks.values()):
torchvision.transforms.transforms forward:226     :5495.1 Mb         return F.normalize(tensor, self.mean, self.std, self.inplace)
selectors select:422                              :5495.1 Mb             if event & ~self._EVENT_WRITE:
multiprocessing.synchronize __exit__:233          :5495.1 Mb         return self._lock.__exit__(*args)
threading __init__:237                            :5495.1 Mb             pass
torch.utils.data._utils.collate default_collate:49:6595.1 Mb         if torch.utils.data.get_worker_info() is not None:
multiprocessing.synchronize __exit__:98           :6595.1 Mb         return self._semlock.__exit__(*args)
threading __init__:238                            :6595.1 Mb         self._waiters = _deque()
torchvision.transforms.functional normalize:265   :6595.1 Mb     if not isinstance(tensor, torch.Tensor):
torch.nn.modules.module __getattr__:769           :6595.1 Mb                 return _parameters[name]
+ torch.nn.modules.module __getattr__:769            (2, 1, 4, 4)         <class 'torch.Tensor'>
+ torch.nn.modules.module __getattr__:769            (2, 513, 4, 4)       <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 512, 7, 7)       <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 512, 8, 8)       <class 'torch.Tensor'>
multiprocessing.util debug:49                     :6595.1 Mb     if _logger:
torch.nn.modules.module _call_impl:727            :6595.1 Mb             result = self.forward(*input, **kwargs)
torch.nn.modules.module _call_impl:724            :6595.1 Mb         if torch._C._get_tracing_state():
selectors select:423                              :6595.1 Mb                 events |= EVENT_READ
torch.overrides <genexpr>:1084                    :6865.1 Mb         type(a) is not torch.Tensor and
threading __init__:786                            :6865.1 Mb         assert group is None, "group argument must be None for now"
multiprocessing.connection _recv:377              :6865.1 Mb         remaining = size
multiprocessing.queues _start_thread:175          :6865.1 Mb                 self._thread, Queue._finalize_join,
multiprocessing.queues put:89                     :6865.1 Mb             self._notempty.notify()
torch.nn.modules.module _call_impl:728            :6865.1 Mb         for hook in itertools.chain(
torch.nn.modules.module _call_impl:718            :6865.1 Mb                 self._forward_pre_hooks.values()):
torch.nn.modules.module __getattr__:769           :6865.1 Mb                 return _parameters[name]
+ torch.nn.modules.module __getattr__:769            (2, 1)               <class 'torch.Tensor'>
+ torch.nn.modules.module __getattr__:769            (2, 512, 32, 32)     <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 1, 4, 4)         <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 512, 4, 4)       <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 513, 4, 4)       <class 'torch.Tensor'>
torch.nn.modules.module _call_impl:717            :6865.1 Mb                 _global_forward_pre_hooks.values(),
torchvision.transforms.transforms forward:617     :8083.1 Mb         return img
multiprocessing.util __init__:199                 :8083.1 Mb         self._kwargs = kwargs or {}
torch.nn.modules.module _call_impl:717            :8083.1 Mb                 _global_forward_pre_hooks.values(),
multiprocessing.connection _recv:378              :8083.1 Mb         while remaining > 0:
importlib._bootstrap _handle_fromlist:1019        :8083.1 Mb     if hasattr(module, '__path__'):
multiprocessing.queues get:102                    :8083.1 Mb                 if block:
torchvision.transforms.functional hflip:457       :8083.1 Mb     if not isinstance(img, torch.Tensor):
threading __init__:227                            :8083.1 Mb             self._release_save = lock._release_save
torchvision.transforms.transforms forward:645     :9199.1 Mb         if torch.rand(1) < self.p:
multiprocessing.queues get:109                    :9199.1 Mb                 self._sem.release()
multiprocessing.queues put:88                     :9199.1 Mb             self._buffer.append(obj)
torchvision.transforms.functional_pil hflip:51    :9199.1 Mb     if not _is_pil_image(img):
torch.tensor <genexpr>:24                         :9199.1 Mb         if not all(type(t) is Tensor for t in args) and has_torch_function(args):
multiprocessing.connection poll:255               :9199.1 Mb         self._check_closed()
threading __init__:229                            :9199.1 Mb             pass
torch.nn.modules.module _call_impl:749            :9199.1 Mb         return result
torch.tensor wrapped:23                           :7475.1 Mb         from torch.overrides import has_torch_function, handle_torch_function
multiprocessing.queues put:89                     :6769.1 Mb             self._notempty.notify()
torchvision.transforms.functional_pil _is_pil_image:16:6769.1 Mb     if accimage is not None:
multiprocessing.queues get:111                    :6769.1 Mb                 self._rlock.release()
torchvision.transforms.transforms __call__:66     :6769.1 Mb         for t in self.transforms:
multiprocessing.connection _check_closed:135      :6769.1 Mb         if self._handle is None:
torch.tensor <genexpr>:24                         :6769.1 Mb         if not all(type(t) is Tensor for t in args) and has_torch_function(args):
threading __init__:230                            :6769.1 Mb         try:
importlib._bootstrap _handle_fromlist:1019        :7923.1 Mb     if hasattr(module, '__path__'):
importlib._bootstrap _handle_fromlist:1044        :6769.1 Mb     return module
dataset __getitem__:29                            :7035.1 Mb                 with self.env.begin(write=False) as txn:
multiprocessing.connection _recv_bytes:408        :7035.1 Mb         size, = struct.unpack("!i", buf.getvalue())
torchvision.transforms.functional normalize:281   :7035.1 Mb         mean = mean.view(-1, 1, 1)
torchvision.transforms.transforms __call__:104    :7035.1 Mb         return F.to_tensor(pic)
torch.overrides <genexpr>:1087                    :7035.1 Mb         for a in relevant_args
importlib._bootstrap _handle_fromlist:1020        :7035.1 Mb         for x in fromlist:
torch.nn.modules.module _call_impl:724            :7035.1 Mb         if torch._C._get_tracing_state():
multiprocessing.connection _recv:379              :7035.1 Mb             chunk = read(handle, remaining)
torch.autograd backward:132                       :7743.1 Mb         allow_unreachable=True)  # allow_unreachable flag
+ torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
+ torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
+ torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- models.stylegan2 forward:250                       (2, 512)             <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 1)               <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 512, 32, 32)     <class 'torch.Tensor'>
dataset __getitem__:30                            :7743.1 Mb                     key = f"{self.resolution}-{str(index).zfill(5)}".encode("utf-8")
multiprocessing.connection _recv_bytes:409        :7743.1 Mb         if maxsize is not None and size > maxsize:
importlib._bootstrap _handle_fromlist:1044        :7743.1 Mb     return module
torchvision.transforms.functional normalize:282   :7743.1 Mb     if std.ndim == 1:
torchvision.transforms.functional to_tensor:63    :7743.1 Mb     if not(F_pil._is_pil_image(pic) or _is_numpy(pic)):
torch.nn.modules.module _call_impl:727            :7743.1 Mb             result = self.forward(*input, **kwargs)
torch.overrides <genexpr>:1084                    :7743.1 Mb         type(a) is not torch.Tensor and
multiprocessing.connection _recv:380              :7743.1 Mb             n = len(chunk)
multiprocessing.connection wait:914               :7279.1 Mb             for obj in object_list:
multiprocessing.connection wait:918               :6179.1 Mb                 deadline = time.monotonic() + timeout
torch.overrides <genexpr>:1084                    :6179.1 Mb         type(a) is not torch.Tensor and
torch.nn.modules.module __getattr__:769           :4999.1 Mb                 return _parameters[name]
+ torch.nn.modules.module __getattr__:769            (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ torch.nn.modules.module __getattr__:769            ()                   <class 'torch.Tensor'>
+ torch.nn.modules.module __getattr__:769            (2, 1)               <class 'torch.Tensor'>
+ torch.nn.modules.module __getattr__:769            (2, 32, 1024, 1024)  <class 'torch.Tensor'>
+ torch.nn.modules.module __getattr__:769            (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
multiprocessing.connection wait:915               :4999.1 Mb                 selector.register(obj, selectors.EVENT_READ)
multiprocessing.connection wait:920               :4999.1 Mb             while True:
torch.overrides <genexpr>:1087                    :4999.1 Mb         for a in relevant_args
selectors register:352                            :6611.1 Mb         key = super().register(fileobj, events, data)
multiprocessing.connection wait:921               :6353.1 Mb                 ready = selector.select(timeout)
torch.overrides <genexpr>:1084                    :6097.1 Mb         type(a) is not torch.Tensor and
torch.autograd grad:204                           :6097.1 Mb         inputs, allow_unused)
+ torch.autograd grad:204                            (2, 1)               <class 'torch.Tensor'>
+ torch.autograd grad:204                            (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 32, 1024, 1024)  <class 'torch.Tensor'>
selectors register:235                            :6097.1 Mb         if (not events) or (events & ~(EVENT_READ | EVENT_WRITE)):
torch.overrides <genexpr>:1087                    :6609.1 Mb         for a in relevant_args
selectors register:238                            :6609.1 Mb         key = SelectorKey(fileobj, self._fileobj_lookup(fileobj), events, data)
torchvision.transforms.functional normalize:275   :9169.1 Mb     dtype = tensor.dtype
selectors _fileobj_lookup:224                     :9169.1 Mb         try:
selectors select:415                              :9169.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :9169.1 Mb             fd_event_list = self._selector.poll(timeout)
torchvision.transforms.functional normalize:265   :8657.1 Mb     if not isinstance(tensor, torch.Tensor):
torch.utils.data._utils.worker _worker_loop:169   :8657.1 Mb         while watchdog.is_alive():
selectors _fileobj_to_fd:36                       :8657.1 Mb         try:
selectors select:415                              :8657.1 Mb             fd_event_list = self._selector.poll(timeout)
torchvision.transforms.functional normalize:268   :7505.1 Mb     if tensor.ndim < 3:
torch.utils.data._utils.worker is_alive:55        :7505.1 Mb             if not self.manager_dead:
selectors _fileobj_to_fd:37                       :7505.1 Mb             fd = int(fileobj.fileno())
selectors select:418                              :7505.1 Mb         for fd, event in fd_event_list:
torch.utils.data._utils.worker _worker_loop:171   :6957.1 Mb                 r = index_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
multiprocessing.connection wait:928               :6957.1 Mb                             return ready
torchvision.transforms.functional normalize:272   :5165.1 Mb     if not inplace:
multiprocessing.connection wait:925               :5169.1 Mb                     if timeout is not None:
multiprocessing.queues get:92                     :7347.1 Mb         if block and timeout is None:
selectors __exit__:203                            :7347.1 Mb         self.close()
selectors register:244                            :5169.1 Mb         self._fd_to_key[key.fd] = key
multiprocessing.queues get:98                     :5169.1 Mb                 deadline = time.monotonic() + timeout
torchvision.transforms.functional normalize:273   :6267.1 Mb         tensor = tensor.clone()
selectors register:245                            :6267.1 Mb         return key
multiprocessing.queues get:97                     :6267.1 Mb             if block:
multiprocessing.connection wait:926               :6267.1 Mb                         timeout = deadline - time.monotonic()
multiprocessing.queues get:99                     :6267.1 Mb             if not self._rlock.acquire(block, timeout):
selectors close:269                               :6267.1 Mb         self._fd_to_key.clear()
multiprocessing.connection wait:921               :6525.1 Mb                 ready = selector.select(timeout)
torch.autograd grad:204                           :7037.1 Mb         inputs, allow_unused)
+ torch.autograd grad:204                            (1,)                 <class 'torch.Tensor'>
torch.overrides has_torch_function:1084           :6525.1 Mb         type(a) is not torch.Tensor and
selectors register:356                            :7037.1 Mb         if events & EVENT_WRITE:
selectors select:405                              :7037.1 Mb         if timeout is None:
torch.overrides has_torch_function:1087           :7293.1 Mb         for a in relevant_args
selectors register:358                            :7295.1 Mb         try:
torch.autograd backward:132                       :7295.1 Mb         allow_unreachable=True)  # allow_unreachable flag
+ torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd grad:204                            (1,)                 <class 'torch.Tensor'>
selectors select:415                              :7295.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:407                              :7295.1 Mb         elif timeout <= 0:
selectors select:415                              :7295.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :7295.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :7295.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :7295.1 Mb             fd_event_list = self._selector.poll(timeout)
multiprocessing.connection wait:925               :5145.1 Mb                     if timeout is not None:
multiprocessing.connection wait:926               :7333.1 Mb                         timeout = deadline - time.monotonic()
torch.autograd backward:132                       :5147.1 Mb         allow_unreachable=True)  # allow_unreachable flag
+ torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
+ torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
+ torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 512)             <class 'torch.Tensor'>
- torch.autograd grad:204                            (2, 1)               <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 1)               <class 'torch.Tensor'>
- torch.autograd grad:204                            (2, 3, 1024, 1024)   <class 'torch.Tensor'>
multiprocessing.connection wait:927               :5147.1 Mb                         if timeout < 0:
selectors select:415                              :5167.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :5167.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :5171.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors register:238                            :5429.1 Mb         key = SelectorKey(fileobj, self._fileobj_lookup(fileobj), events, data)
op.upfirdn2d forward:119                          :5429.1 Mb             input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1
+ op.upfirdn2d forward:119                           (64, 1025, 1025, 1)  <class 'torch.Tensor'>
+ op.upfirdn2d forward:119                           (64, 1024, 1024, 1)  <class 'torch.Tensor'>
+ op.upfirdn2d forward:119                           (4, 4)               <class 'torch.Tensor'>
+ op.upfirdn2d forward:119                           (2, 32, 1024, 1024)  <class 'torch.Tensor'>
+ op.upfirdn2d forward:119                           (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ op.upfirdn2d forward:119                           (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
selectors select:418                              :5429.1 Mb         for fd, event in fd_event_list:
selectors select:418                              :5429.1 Mb         for fd, event in fd_event_list:
selectors select:418                              :5429.1 Mb         for fd, event in fd_event_list:
selectors select:415                              :5429.1 Mb             fd_event_list = self._selector.poll(timeout)
multiprocessing.connection _check_closed:135      :5813.1 Mb         if self._handle is None:
models.stylegan2 forward:633                      :5813.1 Mb             out = (out + skip) / math.sqrt(2)
+ models.stylegan2 forward:633                       (2, 64, 512, 512)    <class 'torch.Tensor'>
- op.upfirdn2d forward:119                           (4, 4)               <class 'torch.Tensor'>
- op.upfirdn2d forward:119                           (64, 1025, 1025, 1)  <class 'torch.Tensor'>
- op.upfirdn2d forward:119                           (64, 1024, 1024, 1)  <class 'torch.Tensor'>
selectors select:415                              :5813.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:428                              :5813.1 Mb         return ready
selectors select:428                              :5813.1 Mb         return ready
selectors select:418                              :5813.1 Mb         for fd, event in fd_event_list:
selectors select:428                              :5813.1 Mb         return ready
selectors select:415                              :5813.1 Mb             fd_event_list = self._selector.poll(timeout)
op.upfirdn2d forward:119                          :6071.1 Mb             input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1
+ op.upfirdn2d forward:119                           (4, 4)               <class 'torch.Tensor'>
+ op.upfirdn2d forward:119                           (128, 511, 511, 1)   <class 'torch.Tensor'>
+ op.upfirdn2d forward:119                           (2, 128, 256, 256)   <class 'torch.Tensor'>
+ op.upfirdn2d forward:119                           (128, 512, 512, 1)   <class 'torch.Tensor'>
- op.upfirdn2d forward:119                           (2, 32, 1024, 1024)  <class 'torch.Tensor'>
selectors register:240                            :6071.1 Mb         if key.fd in self._fd_to_key:
selectors select:418                              :6071.1 Mb         for fd, event in fd_event_list:
multiprocessing.connection wait:922               :6071.1 Mb                 if ready:
multiprocessing.connection wait:922               :6071.1 Mb                 if ready:
selectors select:428                              :6071.1 Mb         return ready
multiprocessing.connection wait:922               :6071.1 Mb                 if ready:
selectors select:415                              :6071.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:418                              :6071.1 Mb         for fd, event in fd_event_list:
selectors register:353                            :6347.1 Mb         poller_events = 0
op.upfirdn2d forward:119                          :6347.1 Mb             input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1
+ op.upfirdn2d forward:119                           (512, 127, 127, 1)   <class 'torch.Tensor'>
+ op.upfirdn2d forward:119                           (512, 128, 128, 1)   <class 'torch.Tensor'>
+ op.upfirdn2d forward:119                           (2, 256, 128, 128)   <class 'torch.Tensor'>
+ op.upfirdn2d forward:119                           (2, 512, 64, 64)     <class 'torch.Tensor'>
- models.stylegan2 forward:633                       (2, 64, 512, 512)    <class 'torch.Tensor'>
- op.upfirdn2d forward:119                           (128, 511, 511, 1)   <class 'torch.Tensor'>
- op.upfirdn2d forward:119                           (2, 128, 256, 256)   <class 'torch.Tensor'>
- op.upfirdn2d forward:119                           (128, 512, 512, 1)   <class 'torch.Tensor'>
selectors select:428                              :6347.1 Mb         return ready
multiprocessing.connection wait:925               :6347.1 Mb                     if timeout is not None:
multiprocessing.connection wait:925               :6347.1 Mb                     if timeout is not None:
multiprocessing.connection wait:922               :6347.1 Mb                 if ready:
multiprocessing.connection wait:925               :6347.1 Mb                     if timeout is not None:
selectors select:418                              :6347.1 Mb         for fd, event in fd_event_list:
selectors select:428                              :6347.1 Mb         return ready
torch.utils.data._utils.worker _worker_loop:171   :6863.1 Mb                 r = index_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
torch.utils.data._utils.worker is_alive:57        :6863.1 Mb             return not self.manager_dead
torch.utils.data._utils.worker _worker_loop:172   :6863.1 Mb             except queue.Empty:
multiprocessing.connection wait:925               :7507.1 Mb                     if timeout is not None:
multiprocessing.connection wait:926               :7507.1 Mb                         timeout = deadline - time.monotonic()
torch.autograd backward:132                       :7507.1 Mb         allow_unreachable=True)  # allow_unreachable flag
- op.upfirdn2d forward:119                           (512, 127, 127, 1)   <class 'torch.Tensor'>
- op.upfirdn2d forward:119                           (512, 128, 128, 1)   <class 'torch.Tensor'>
- op.upfirdn2d forward:119                           (4, 4)               <class 'torch.Tensor'>
- op.upfirdn2d forward:119                           (2, 256, 128, 128)   <class 'torch.Tensor'>
- op.upfirdn2d forward:119                           (2, 512, 64, 64)     <class 'torch.Tensor'>
selectors select:428                              :7507.1 Mb         return ready
multiprocessing.connection wait:922               :7507.1 Mb                 if ready:
multiprocessing.queues get:92                     :7507.1 Mb         if block and timeout is None:
torch.utils.data._utils.worker _worker_loop:170   :7507.1 Mb             try:
torch.utils.data._utils.worker _worker_loop:173   :7507.1 Mb                 continue
selectors select:415                              :7507.1 Mb             fd_event_list = self._selector.poll(timeout)
torchvision.transforms.functional normalize:281   :7799.1 Mb         mean = mean.view(-1, 1, 1)
torch.autograd backward:132                       :7799.1 Mb         allow_unreachable=True)  # allow_unreachable flag
+ torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- op.upfirdn2d forward:119                           (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            ()                   <class 'torch.Tensor'>
- op.upfirdn2d forward:119                           (2, 512)             <class 'torch.Tensor'>
- torch.nn.modules.module __getattr__:769            (2, 3, 1024, 1024)   <class 'torch.Tensor'>
selectors select:415                              :7799.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :7799.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :7799.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :7799.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :7799.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :7799.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :7799.1 Mb             fd_event_list = self._selector.poll(timeout)
multiprocessing.connection wait:914               :8409.1 Mb             for obj in object_list:
torch.autograd backward:132                       :8409.1 Mb         allow_unreachable=True)  # allow_unreachable flag
selectors select:415                              :8409.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :8409.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :8409.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :8409.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :8409.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :8409.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :8409.1 Mb             fd_event_list = self._selector.poll(timeout)
torch.nn.functional grid_sample:3391              :9085.1 Mb     return torch.grid_sampler(input, grid, mode_enum, padding_mode_enum, align_corners)
+ torch.nn.functional grid_sample:3391               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ torch.nn.functional grid_sample:3391               ()                   <class 'torch.Tensor'>
+ torch.nn.functional grid_sample:3391               (2, 3, 2720, 2720)   <class 'torch.Tensor'>
+ torch.nn.functional grid_sample:3391               (12, 12)             <class 'torch.Tensor'>
+ torch.nn.functional grid_sample:3391               (1,)                 <class 'torch.Tensor'>
+ torch.nn.functional grid_sample:3391               (2, 1)               <class 'torch.Tensor'>
+ torch.nn.functional grid_sample:3391               (2, 5429, 5429, 2)   <class 'torch.Tensor'>
+ torch.nn.functional grid_sample:3391               (2, 512)             <class 'torch.Tensor'>
+ torch.nn.functional grid_sample:3391               (2, 3, 5429, 5429)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
torchvision.transforms.functional_pil _is_pil_image:19:9085.1 Mb         return isinstance(img, Image.Image)
selectors select:415                              :9085.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :9085.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :9085.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :9085.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :9085.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :9085.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :9085.1 Mb             fd_event_list = self._selector.poll(timeout)
multiprocessing.queues get:104                    :9749.1 Mb                     if not self._poll(timeout):
+ multiprocessing.queues get:104                     (1,)                 <class 'torch.Tensor'>
+ multiprocessing.queues get:104                     (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ multiprocessing.queues get:104                     ()                   <class 'torch.Tensor'>
+ multiprocessing.queues get:104                     (2, 512)             <class 'torch.Tensor'>
+ multiprocessing.queues get:104                     (2, 1)               <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               ()                   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 2720, 2720)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (12, 12)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (1,)                 <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 1)               <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 5429, 5429, 2)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 512)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 5429, 5429)   <class 'torch.Tensor'>
selectors select:415                              :9749.1 Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               ()                   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 2720, 2720)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (12, 12)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (1,)                 <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 1)               <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 5429, 5429, 2)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 512)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 5429, 5429)   <class 'torch.Tensor'>
torch.autograd backward:132                       :9749.1 Mb         allow_unreachable=True)  # allow_unreachable flag
+ torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
+ torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
+ torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
+ torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               ()                   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 2720, 2720)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (12, 12)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (1,)                 <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 1)               <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 5429, 5429, 2)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 512)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 5429, 5429)   <class 'torch.Tensor'>
selectors select:415                              :9749.1 Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               ()                   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 2720, 2720)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (12, 12)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (1,)                 <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 1)               <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 5429, 5429, 2)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 512)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 5429, 5429)   <class 'torch.Tensor'>
selectors select:415                              :9749.1 Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               ()                   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 2720, 2720)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (12, 12)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (1,)                 <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 1)               <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 5429, 5429, 2)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 512)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 5429, 5429)   <class 'torch.Tensor'>
selectors select:415                              :9749.1 Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               ()                   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 2720, 2720)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (12, 12)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (1,)                 <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 1)               <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 5429, 5429, 2)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 512)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 5429, 5429)   <class 'torch.Tensor'>
selectors select:415                              :9749.1 Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               ()                   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 2720, 2720)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (12, 12)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (1,)                 <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 1)               <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 5429, 5429, 2)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 512)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 5429, 5429)   <class 'torch.Tensor'>
selectors select:415                              :9749.1 Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               ()                   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 2720, 2720)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (12, 12)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (1,)                 <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 1)               <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 5429, 5429, 2)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 512)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 5429, 5429)   <class 'torch.Tensor'>
selectors select:415                              :9749.1 Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               ()                   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 2720, 2720)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (12, 12)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (1,)                 <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 1)               <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 5429, 5429, 2)   <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 512)             <class 'torch.Tensor'>
- torch.nn.functional grid_sample:3391               (2, 3, 5429, 5429)   <class 'torch.Tensor'>
torch.autograd backward:132                       :10545.1Mb         allow_unreachable=True)  # allow_unreachable flag
selectors select:415                              :10545.1Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
selectors select:415                              :10545.1Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
selectors select:415                              :10545.1Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
selectors select:415                              :10545.1Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
selectors select:415                              :10545.1Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
selectors select:415                              :10545.1Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
selectors select:415                              :10545.1Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
selectors select:415                              :10545.1Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
selectors select:415                              :8759.1 Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
torch.autograd backward:132                       :8759.1 Mb         allow_unreachable=True)  # allow_unreachable flag
selectors select:415                              :8759.1 Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
selectors select:415                              :8759.1 Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
selectors select:415                              :8759.1 Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
selectors select:415                              :8769.1 Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
selectors select:415                              :8769.1 Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
selectors select:415                              :8769.1 Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
selectors select:415                              :8769.1 Mb             fd_event_list = self._selector.poll(timeout)
+ selectors select:415                               ()                   <class 'torch.Tensor'>
+ selectors select:415                               (1,)                 <class 'torch.Tensor'>
+ selectors select:415                               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ selectors select:415                               (2, 512)             <class 'torch.Tensor'>
+ selectors select:415                               (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
torch.nn.functional grid_sample:3391              :9625.1 Mb     return torch.grid_sampler(input, grid, mode_enum, padding_mode_enum, align_corners)
+ torch.nn.functional grid_sample:3391               (2, 3, 1024, 1024)   <class 'torch.Tensor'>
+ torch.nn.functional grid_sample:3391               ()                   <class 'torch.Tensor'>
+ torch.nn.functional grid_sample:3391               (2, 3, 3062, 3062)   <class 'torch.Tensor'>
+ torch.nn.functional grid_sample:3391               (2, 3, 6113, 6113)   <class 'torch.Tensor'>
+ torch.nn.functional grid_sample:3391               (12, 12)             <class 'torch.Tensor'>
+ torch.nn.functional grid_sample:3391               (2, 1)               <class 'torch.Tensor'>
+ torch.nn.functional grid_sample:3391               (2, 6113, 6113, 2)   <class 'torch.Tensor'>
+ torch.nn.functional grid_sample:3391               (2, 512)             <class 'torch.Tensor'>
+ torch.nn.functional grid_sample:3391               (1,)                 <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 512)             <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 3, 1024, 1024)   <class 'torch.Tensor'>
- torch.autograd backward:132                        (2, 1)               <class 'torch.Tensor'>
- torch.autograd backward:132                        ()                   <class 'torch.Tensor'>
selectors select:428                              :9625.1 Mb         return ready
threading _is_owned:258                           :9625.1 Mb         if self._lock.acquire(0):
torchvision.transforms.functional to_tensor:87    :9625.1 Mb     if pic.mode == 'I':
selectors select:415                              :9625.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :9625.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :9625.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :9625.1 Mb             fd_event_list = self._selector.poll(timeout)
selectors select:415                              :9625.1 Mb             fd_event_list = self._selector.poll(timeout)
